{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BcellEpitope.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shuvam1024/BcellEpitope/blob/main/BcellEpitope.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G9gRlu8zmDj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee30be71-2010-4cab-a89c-9e67a730d9e9"
      },
      "source": [
        "#@title Run on TensorFlow 2.x\n",
        "%tensorflow_version 2.x\n",
        " \n",
        "\n",
        "# To determine which version you're using:\n",
        "!pip show tensorflow\n",
        "\n",
        "# For the current version: \n",
        "# !pip install --upgrade tensorflow\n",
        "\n",
        "# For the latest nightly build:\n",
        "# !pip install tf-nightly"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "Name: tensorflow\n",
            "Version: 2.8.2+zzzcolab20220719082949\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: termcolor, wrapt, tensorflow-estimator, keras, gast, absl-py, protobuf, astunparse, libclang, opt-einsum, numpy, h5py, keras-preprocessing, flatbuffers, typing-extensions, google-pasta, setuptools, grpcio, tensorflow-io-gcs-filesystem, six, tensorboard\n",
            "Required-by: kapre\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykP5CcrI0L2p"
      },
      "source": [
        "#@title Import relevant modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "# print(tf.__version__)\n",
        "\n",
        "# The following lines adjust the granularity of reporting. \n",
        "pd.options.display.max_rows = 500\n",
        "pd.options.display.float_format = \"{:.1f}\".format\n",
        "\n",
        "# The following line improves formatting when ouputting NumPy arrays.\n",
        "# np.set_printoptions(linewidth = 200)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJufk-I80RoW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d6cfdd1-9c0b-4df1-e94b-16dad5e2b6f1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6uq5SBT0vZq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "2f8d793d-6c6c-4165-b6e7-e255b771b5d5"
      },
      "source": [
        "#!ls gdrive/\"My Drive\"/\"Colab Notebooks\"\n",
        "# data_df = pd.read_csv(\"gdrive/My Drive/Colab Notebooks/input_bcell.csv\", sep=\",\")\n",
        "data1_df = pd.read_csv(\"gdrive/My Drive/Colab Notebooks/input_bcell.csv\", sep=\",\")\n",
        "data2_df = pd.read_csv(\"gdrive/My Drive/Colab Notebooks/input_sars.csv\", sep=\",\")\n",
        "data_df = pd.concat([data1_df, data2_df], ignore_index=True)\n",
        "data_df = data_df.reindex(np.random.permutation(data_df.index)) # shuffle the examples\n",
        "data_df.describe()\n",
        "\n",
        "#data_df_num.corr()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       start_position  end_position  chou_fasman   emini  kolaskar_tongaonkar  \\\n",
              "count         14907.0       14907.0      14907.0 14907.0              14907.0   \n",
              "mean            308.8         319.5          1.0     1.1                  1.0   \n",
              "std             358.4         358.6          0.1     1.8                  0.1   \n",
              "min               1.0           6.0          0.5     0.0                  0.8   \n",
              "25%              86.0          96.0          0.9     0.2                  1.0   \n",
              "50%             197.0         208.0          1.0     0.6                  1.0   \n",
              "75%             400.0         411.0          1.1     1.2                  1.1   \n",
              "max            3079.0        3086.0          1.5    40.6                  1.3   \n",
              "\n",
              "       parker  isoelectric_point  aromaticity  hydrophobicity  stability  \\\n",
              "count 14907.0            14907.0      14907.0         14907.0    14907.0   \n",
              "mean      1.8                7.0          0.1            -0.4       43.3   \n",
              "std       2.0                1.9          0.0             0.4       16.5   \n",
              "min      -9.0                3.7          0.0            -2.0        5.4   \n",
              "25%       0.6                5.6          0.1            -0.6       31.7   \n",
              "50%       1.8                6.4          0.1            -0.3       41.9   \n",
              "75%       3.0                8.7          0.1            -0.2       49.1   \n",
              "max       9.1               12.2          0.2             1.3      137.0   \n",
              "\n",
              "       target  \n",
              "count 14907.0  \n",
              "mean      0.3  \n",
              "std       0.4  \n",
              "min       0.0  \n",
              "25%       0.0  \n",
              "50%       0.0  \n",
              "75%       1.0  \n",
              "max       1.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8f3251cb-bfa7-4b22-8337-f4d169a9d9ad\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start_position</th>\n",
              "      <th>end_position</th>\n",
              "      <th>chou_fasman</th>\n",
              "      <th>emini</th>\n",
              "      <th>kolaskar_tongaonkar</th>\n",
              "      <th>parker</th>\n",
              "      <th>isoelectric_point</th>\n",
              "      <th>aromaticity</th>\n",
              "      <th>hydrophobicity</th>\n",
              "      <th>stability</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>308.8</td>\n",
              "      <td>319.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>43.3</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>358.4</td>\n",
              "      <td>358.6</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>16.5</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>-9.0</td>\n",
              "      <td>3.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>5.4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>86.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>5.6</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.6</td>\n",
              "      <td>31.7</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>197.0</td>\n",
              "      <td>208.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>6.4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>41.9</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>400.0</td>\n",
              "      <td>411.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>49.1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3079.0</td>\n",
              "      <td>3086.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>40.6</td>\n",
              "      <td>1.3</td>\n",
              "      <td>9.1</td>\n",
              "      <td>12.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>137.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8f3251cb-bfa7-4b22-8337-f4d169a9d9ad')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8f3251cb-bfa7-4b22-8337-f4d169a9d9ad button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8f3251cb-bfa7-4b22-8337-f4d169a9d9ad');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rawNuA213j86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "32d92e16-1c20-4d41-ed79-f9e0ec7b2c76"
      },
      "source": [
        "#@title Add synthetic features\n",
        "\n",
        "def add_synthetic_features(data):\n",
        "  data[\"peptide_length\"] = data[\"end_position\"] - data[\"start_position\"] + 1\n",
        "  data[\"hs_ratio\"] = data[\"hydrophobicity\"] / data[\"stability\"]\n",
        "  return data\n",
        "\n",
        "data_df = add_synthetic_features(data_df)\n",
        "# Examine correlations\n",
        "data_df.corr()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     start_position  end_position  chou_fasman  emini  \\\n",
              "start_position                  1.0           1.0          0.0    0.0   \n",
              "end_position                    1.0           1.0          0.0    0.0   \n",
              "chou_fasman                     0.0           0.0          1.0    0.2   \n",
              "emini                           0.0           0.0          0.2    1.0   \n",
              "kolaskar_tongaonkar            -0.0          -0.0         -0.4   -0.4   \n",
              "parker                         -0.0          -0.0          0.6    0.4   \n",
              "isoelectric_point              -0.3          -0.3          0.0   -0.0   \n",
              "aromaticity                     0.3           0.3          0.0    0.0   \n",
              "hydrophobicity                  0.0           0.0         -0.1    0.0   \n",
              "stability                      -0.1          -0.1          0.1   -0.0   \n",
              "target                         -0.1          -0.1          0.1    0.1   \n",
              "peptide_length                  0.0           0.0          0.0    0.1   \n",
              "hs_ratio                       -0.0          -0.0         -0.1    0.0   \n",
              "\n",
              "                     kolaskar_tongaonkar  parker  isoelectric_point  \\\n",
              "start_position                      -0.0    -0.0               -0.3   \n",
              "end_position                        -0.0    -0.0               -0.3   \n",
              "chou_fasman                         -0.4     0.6                0.0   \n",
              "emini                               -0.4     0.4               -0.0   \n",
              "kolaskar_tongaonkar                  1.0    -0.7               -0.0   \n",
              "parker                              -0.7     1.0               -0.0   \n",
              "isoelectric_point                   -0.0    -0.0                1.0   \n",
              "aromaticity                          0.1    -0.2               -0.2   \n",
              "hydrophobicity                       0.3    -0.3               -0.2   \n",
              "stability                           -0.0     0.1                0.2   \n",
              "target                              -0.0     0.0               -0.1   \n",
              "peptide_length                       0.0    -0.0               -0.1   \n",
              "hs_ratio                             0.2    -0.2               -0.2   \n",
              "\n",
              "                     aromaticity  hydrophobicity  stability  target  \\\n",
              "start_position               0.3             0.0       -0.1    -0.1   \n",
              "end_position                 0.3             0.0       -0.1    -0.1   \n",
              "chou_fasman                  0.0            -0.1        0.1     0.1   \n",
              "emini                        0.0             0.0       -0.0     0.1   \n",
              "kolaskar_tongaonkar          0.1             0.3       -0.0    -0.0   \n",
              "parker                      -0.2            -0.3        0.1     0.0   \n",
              "isoelectric_point           -0.2            -0.2        0.2    -0.1   \n",
              "aromaticity                  1.0             0.3       -0.3     0.0   \n",
              "hydrophobicity               0.3             1.0       -0.4     0.1   \n",
              "stability                   -0.3            -0.4        1.0     0.0   \n",
              "target                       0.0             0.1        0.0     1.0   \n",
              "peptide_length               0.2             0.1       -0.0     0.1   \n",
              "hs_ratio                     0.2             0.8        0.0     0.1   \n",
              "\n",
              "                     peptide_length  hs_ratio  \n",
              "start_position                  0.0      -0.0  \n",
              "end_position                    0.0      -0.0  \n",
              "chou_fasman                     0.0      -0.1  \n",
              "emini                           0.1       0.0  \n",
              "kolaskar_tongaonkar             0.0       0.2  \n",
              "parker                         -0.0      -0.2  \n",
              "isoelectric_point              -0.1      -0.2  \n",
              "aromaticity                     0.2       0.2  \n",
              "hydrophobicity                  0.1       0.8  \n",
              "stability                      -0.0       0.0  \n",
              "target                          0.1       0.1  \n",
              "peptide_length                  1.0       0.1  \n",
              "hs_ratio                        0.1       1.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-828e82f2-d017-4d20-b6fc-a0e86379f1de\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start_position</th>\n",
              "      <th>end_position</th>\n",
              "      <th>chou_fasman</th>\n",
              "      <th>emini</th>\n",
              "      <th>kolaskar_tongaonkar</th>\n",
              "      <th>parker</th>\n",
              "      <th>isoelectric_point</th>\n",
              "      <th>aromaticity</th>\n",
              "      <th>hydrophobicity</th>\n",
              "      <th>stability</th>\n",
              "      <th>target</th>\n",
              "      <th>peptide_length</th>\n",
              "      <th>hs_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>start_position</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>end_position</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chou_fasman</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emini</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>0.4</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>kolaskar_tongaonkar</th>\n",
              "      <td>-0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.7</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>parker</th>\n",
              "      <td>-0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.4</td>\n",
              "      <td>-0.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>-0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>isoelectric_point</th>\n",
              "      <td>-0.3</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>aromaticity</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hydrophobicity</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stability</th>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>peptide_length</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hs_ratio</th>\n",
              "      <td>-0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-828e82f2-d017-4d20-b6fc-a0e86379f1de')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-828e82f2-d017-4d20-b6fc-a0e86379f1de button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-828e82f2-d017-4d20-b6fc-a0e86379f1de');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px4FveXE1VDo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "656da6ce-d710-442a-85ad-4f0f4c5e1c5e"
      },
      "source": [
        "#@title Augment data\n",
        "\n",
        "def convert_raw_to_zscores(data):\n",
        "  # Calculate the Z-scores of each column in the data set:\n",
        "  data_mean = data.mean()\n",
        "  data_std = data.std()\n",
        "  data_norm = (data - data_mean)/data_std\n",
        "  return data_norm\n",
        "\n",
        "def add_normalized_features(data):\n",
        "  data[\"chou_fasman_norm\"] = convert_raw_to_zscores(data[\"chou_fasman\"])\n",
        "  data[\"emini_norm\"] = convert_raw_to_zscores(data[\"emini\"])\n",
        "  data[\"kolaskar_tongaonkar_norm\"] = convert_raw_to_zscores(data[\"kolaskar_tongaonkar\"])\n",
        "  data[\"parker_norm\"] = convert_raw_to_zscores(data_df[\"parker\"])\n",
        "  data[\"isoelectric_point_norm\"] = convert_raw_to_zscores(data[\"isoelectric_point\"])\n",
        "  data[\"aromaticity_norm\"] = convert_raw_to_zscores(data[\"aromaticity\"])\n",
        "  data[\"hydrophobicity_norm\"] = convert_raw_to_zscores(data[\"hydrophobicity\"])\n",
        "  data[\"stability_norm\"] = convert_raw_to_zscores(data[\"stability\"])  \n",
        "  data[\"peptide_length_norm\"] = convert_raw_to_zscores(data[\"peptide_length\"])\n",
        "  data[\"start_position_norm\"] = convert_raw_to_zscores(data[\"start_position\"])\n",
        "  return data\n",
        "\n",
        "data_df = add_normalized_features(data_df)\n",
        "data_df.describe()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       start_position  end_position  chou_fasman   emini  kolaskar_tongaonkar  \\\n",
              "count         14907.0       14907.0      14907.0 14907.0              14907.0   \n",
              "mean            308.8         319.5          1.0     1.1                  1.0   \n",
              "std             358.4         358.6          0.1     1.8                  0.1   \n",
              "min               1.0           6.0          0.5     0.0                  0.8   \n",
              "25%              86.0          96.0          0.9     0.2                  1.0   \n",
              "50%             197.0         208.0          1.0     0.6                  1.0   \n",
              "75%             400.0         411.0          1.1     1.2                  1.1   \n",
              "max            3079.0        3086.0          1.5    40.6                  1.3   \n",
              "\n",
              "       parker  isoelectric_point  aromaticity  hydrophobicity  stability  ...  \\\n",
              "count 14907.0            14907.0      14907.0         14907.0    14907.0  ...   \n",
              "mean      1.8                7.0          0.1            -0.4       43.3  ...   \n",
              "std       2.0                1.9          0.0             0.4       16.5  ...   \n",
              "min      -9.0                3.7          0.0            -2.0        5.4  ...   \n",
              "25%       0.6                5.6          0.1            -0.6       31.7  ...   \n",
              "50%       1.8                6.4          0.1            -0.3       41.9  ...   \n",
              "75%       3.0                8.7          0.1            -0.2       49.1  ...   \n",
              "max       9.1               12.2          0.2             1.3      137.0  ...   \n",
              "\n",
              "       chou_fasman_norm  emini_norm  kolaskar_tongaonkar_norm  parker_norm  \\\n",
              "count           14907.0     14907.0                   14907.0      14907.0   \n",
              "mean                0.0        -0.0                      -0.0          0.0   \n",
              "std                 1.0         1.0                       1.0          1.0   \n",
              "min                -3.7        -0.6                      -3.4         -5.5   \n",
              "25%                -0.7        -0.5                      -0.7         -0.6   \n",
              "50%                -0.0        -0.3                      -0.0          0.0   \n",
              "75%                 0.6         0.1                       0.6          0.6   \n",
              "max                 4.5        21.6                       4.4          3.8   \n",
              "\n",
              "       isoelectric_point_norm  aromaticity_norm  hydrophobicity_norm  \\\n",
              "count                 14907.0           14907.0              14907.0   \n",
              "mean                     -0.0              -0.0                  0.0   \n",
              "std                       1.0               1.0                  1.0   \n",
              "min                      -1.8              -2.9                 -4.0   \n",
              "25%                      -0.8              -0.6                 -0.5   \n",
              "50%                      -0.3              -0.0                  0.2   \n",
              "75%                       0.9               0.6                  0.5   \n",
              "max                       2.8               4.0                  4.2   \n",
              "\n",
              "       stability_norm  peptide_length_norm  start_position_norm  \n",
              "count         14907.0              14907.0              14907.0  \n",
              "mean              0.0                  0.0                 -0.0  \n",
              "std               1.0                  1.0                  1.0  \n",
              "min              -2.3                 -1.2                 -0.9  \n",
              "25%              -0.7                 -0.5                 -0.6  \n",
              "50%              -0.1                 -0.3                 -0.3  \n",
              "75%               0.3                  0.6                  0.3  \n",
              "max               5.7                 68.4                  7.7  \n",
              "\n",
              "[8 rows x 23 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b2884e66-6db6-44c0-a0ff-ebdc987341b5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start_position</th>\n",
              "      <th>end_position</th>\n",
              "      <th>chou_fasman</th>\n",
              "      <th>emini</th>\n",
              "      <th>kolaskar_tongaonkar</th>\n",
              "      <th>parker</th>\n",
              "      <th>isoelectric_point</th>\n",
              "      <th>aromaticity</th>\n",
              "      <th>hydrophobicity</th>\n",
              "      <th>stability</th>\n",
              "      <th>...</th>\n",
              "      <th>chou_fasman_norm</th>\n",
              "      <th>emini_norm</th>\n",
              "      <th>kolaskar_tongaonkar_norm</th>\n",
              "      <th>parker_norm</th>\n",
              "      <th>isoelectric_point_norm</th>\n",
              "      <th>aromaticity_norm</th>\n",
              "      <th>hydrophobicity_norm</th>\n",
              "      <th>stability_norm</th>\n",
              "      <th>peptide_length_norm</th>\n",
              "      <th>start_position_norm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>...</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "      <td>14907.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>308.8</td>\n",
              "      <td>319.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>43.3</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>358.4</td>\n",
              "      <td>358.6</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>16.5</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>-9.0</td>\n",
              "      <td>3.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>5.4</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.7</td>\n",
              "      <td>-0.6</td>\n",
              "      <td>-3.4</td>\n",
              "      <td>-5.5</td>\n",
              "      <td>-1.8</td>\n",
              "      <td>-2.9</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>-2.3</td>\n",
              "      <td>-1.2</td>\n",
              "      <td>-0.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>86.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>5.6</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.6</td>\n",
              "      <td>31.7</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.7</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-0.7</td>\n",
              "      <td>-0.6</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>-0.6</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-0.7</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>197.0</td>\n",
              "      <td>208.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>6.4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>41.9</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>-0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>400.0</td>\n",
              "      <td>411.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>49.1</td>\n",
              "      <td>...</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3079.0</td>\n",
              "      <td>3086.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>40.6</td>\n",
              "      <td>1.3</td>\n",
              "      <td>9.1</td>\n",
              "      <td>12.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>137.0</td>\n",
              "      <td>...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>21.6</td>\n",
              "      <td>4.4</td>\n",
              "      <td>3.8</td>\n",
              "      <td>2.8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.2</td>\n",
              "      <td>5.7</td>\n",
              "      <td>68.4</td>\n",
              "      <td>7.7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows  23 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2884e66-6db6-44c0-a0ff-ebdc987341b5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b2884e66-6db6-44c0-a0ff-ebdc987341b5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b2884e66-6db6-44c0-a0ff-ebdc987341b5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Lsgd6JxNUsA"
      },
      "source": [
        "#@title Split into training and validation sets\n",
        "\n",
        "def train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None):\n",
        "    np.random.seed(seed)\n",
        "    perm = np.random.permutation(df.index)\n",
        "    m = len(df.index)\n",
        "    train_end = int(train_percent * m)\n",
        "    validate_end = int(validate_percent * m) + train_end\n",
        "    train = df.iloc[perm[:train_end]]\n",
        "    validate = df.iloc[perm[train_end:validate_end]]\n",
        "    test = df.iloc[perm[validate_end:]]\n",
        "    return train, validate, test\n",
        "\n",
        "def train_test_split(df, train_percent=.7, seed=None):\n",
        "    np.random.seed(seed)\n",
        "    perm = np.random.permutation(df.index)\n",
        "    m = len(df.index)\n",
        "    train_end = int(train_percent * m)\n",
        "    train = df.iloc[perm[:train_end]]\n",
        "    test = df.iloc[perm[train_end:]]\n",
        "    return train, test\n",
        "\n",
        "train_df, test_df = train_test_split(data_df, 0.7)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P60s8g9z-0M2"
      },
      "source": [
        "#@title Define the input features layer\n",
        "\n",
        "def define_feature_layer():\n",
        "  # Create an empty list that will eventually hold all created feature columns.\n",
        "  feature_columns = [] \n",
        "\n",
        "  feature_columns.append(tf.feature_column.numeric_column(\"start_position_norm\"))\n",
        "  feature_columns.append(tf.feature_column.numeric_column(\"peptide_length_norm\"))\n",
        "  feature_columns.append(tf.feature_column.numeric_column(\"chou_fasman_norm\"))\n",
        "  feature_columns.append(tf.feature_column.numeric_column(\"emini_norm\"))\n",
        "  feature_columns.append(tf.feature_column.numeric_column(\"kolaskar_tongaonkar_norm\"))\n",
        "  feature_columns.append(tf.feature_column.numeric_column(\"parker_norm\"))\n",
        "  feature_columns.append(tf.feature_column.numeric_column(\"isoelectric_point_norm\"))\n",
        "  feature_columns.append(tf.feature_column.numeric_column(\"aromaticity_norm\"))\n",
        "  feature_columns.append(tf.feature_column.numeric_column(\"hydrophobicity_norm\"))\n",
        "  feature_columns.append(tf.feature_column.numeric_column(\"stability_norm\"))\n",
        "  \n",
        "  # Convert the list of feature columns into a layer that will later be fed into\n",
        "  # the model. \n",
        "  my_feature_layer = layers.DenseFeatures(feature_columns)\n",
        "  return my_feature_layer\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDvYn5n-DP03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb634260-c166-4f98-9bc2-6e0d3931754e"
      },
      "source": [
        "#@title Define the plotting function.\n",
        "def plot_curve(epochs, hist, list_of_metrics):\n",
        "  \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n",
        "  # list_of_metrics should be one of the names shown in:\n",
        "  # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#define_the_model_and_metrics  \n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Value\")\n",
        "\n",
        "  for m in list_of_metrics:\n",
        "    x = hist[m]\n",
        "    plt.plot(epochs[1:], x[1:], label=m)\n",
        "\n",
        "  plt.legend()\n",
        "\n",
        "print(\"Defined the plot_curve function.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined the plot_curve function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsURZSOJB1Jh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef052ffd-0774-400c-dd7c-e7106c2a6b40"
      },
      "source": [
        "#@title Define the functions that create and train a model.\n",
        "def create_model(my_learning_rate, feature_layer, my_metrics):\n",
        "  \"\"\"Create and compile a simple classification model.\"\"\"\n",
        "  # Most simple tf.keras models are sequential.\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Add the feature layer (the list of features and how they are represented)\n",
        "  # to the model.\n",
        "  model.add(feature_layer)\n",
        "\n",
        "  # Define the first hidden layer.   \n",
        "  model.add(tf.keras.layers.Dense(units=20, \n",
        "                                  activation='tanh',\n",
        "                                  #kernel_regularizer=tf.keras.regularizers.l2(l=0.001), \n",
        "                                  name='Hidden1'))\n",
        "  \n",
        "  # Define the second hidden layer. \n",
        "  model.add(tf.keras.layers.Dense(units=12, \n",
        "                                  activation='tanh',\n",
        "                                  #kernel_regularizer=tf.keras.regularizers.l2(l=0.001), \n",
        "                                  name='Hidden2'))\n",
        "  \n",
        "  # Define the second hidden layer. \n",
        "  model.add(tf.keras.layers.Dense(units=6, \n",
        "                                  activation='tanh',\n",
        "                                  #kernel_regularizer=tf.keras.regularizers.l2(l=0.001), \n",
        "                                  name='Hidden3'))\n",
        "  \n",
        "\n",
        "  # Funnel the regression value through a sigmoid function.\n",
        "  model.add(tf.keras.layers.Dense(units=1, \n",
        "                                  activation=tf.sigmoid, \n",
        "                                  name='Output'))\n",
        "\n",
        "  # Call the compile method to construct the layers into a model that\n",
        "  # TensorFlow can execute.  Notice that we're using a different loss\n",
        "  # function for classification than for regression.    \n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),                                                   \n",
        "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                metrics=my_metrics)\n",
        "\n",
        "  return model        \n",
        "\n",
        "\n",
        "def train_model(model, dataset, epochs, label_name,\n",
        "                batch_size=None, shuffle=True):\n",
        "  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
        "\n",
        "  # The x parameter of tf.keras.Model.fit can be a list of arrays, where\n",
        "  # each array contains the data for one feature.  Here, we're passing\n",
        "  # every column in the dataset. Note that the feature_layer will filter\n",
        "  # away most of those columns, leaving only the desired columns and their\n",
        "  # representations as features.\n",
        "  features = {name:np.array(value) for name, value in dataset.items()}\n",
        "  label = np.array(features.pop(label_name)) \n",
        "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
        "                      epochs=epochs, shuffle=shuffle)\n",
        "  \n",
        "  # The list of epochs is stored separately from the rest of history.\n",
        "  epochs = history.epoch\n",
        "\n",
        "  # Isolate the classification metric for each epoch.\n",
        "  hist = pd.DataFrame(history.history)\n",
        "\n",
        "  return epochs, hist  \n",
        "\n",
        "print(\"Defined the create_model and train_model functions.\") "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined the create_model and train_model functions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxbncab3DfNA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5a56f74a-8639-4d41-d467-465309a0dc78"
      },
      "source": [
        "#@title Train the model on the training set.\n",
        "\n",
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.001\n",
        "epochs = 400\n",
        "batch_size = 100\n",
        "classification_threshold = 0.25\n",
        "label_name = \"target\"\n",
        "\n",
        "# Here is the updated definition of METRICS:\n",
        "my_metrics = [\n",
        "      tf.keras.metrics.BinaryAccuracy(threshold=classification_threshold,\n",
        "                                      name='accuracy'\n",
        "                                      ),\n",
        "      tf.keras.metrics.Precision(thresholds=classification_threshold,\n",
        "                                 name='precision' \n",
        "                                 ),\n",
        "      tf.keras.metrics.Recall(thresholds=classification_threshold,\n",
        "                              name=\"recall\"),\n",
        "      tf.keras.metrics.AUC(num_thresholds=100, name='auc')\n",
        "]\n",
        "\n",
        "my_feature_layer = define_feature_layer()\n",
        "# Print the first 3 and last 3 rows of the feature_layer's output when applied\n",
        "# to train_df_num_norm:\n",
        "my_feature_layer(dict(train_df))\n",
        "\n",
        "# Establish the model's topography.\n",
        "my_model = create_model(learning_rate, my_feature_layer, my_metrics)\n",
        "\n",
        "# Train the model on the training set.\n",
        "epochs, hist = train_model(my_model, train_df, epochs, \n",
        "                           label_name, batch_size)\n",
        "\n",
        "# Plot metrics vs. epochs\n",
        "list_of_metrics_to_plot = ['accuracy', \"precision\", \"recall\"] \n",
        "plot_curve(epochs, hist, list_of_metrics_to_plot)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'parent_protein_id': <tf.Tensor 'IteratorGetNext:14' shape=(None,) dtype=string>, 'protein_seq': <tf.Tensor 'IteratorGetNext:20' shape=(None,) dtype=string>, 'start_position': <tf.Tensor 'IteratorGetNext:23' shape=(None,) dtype=int64>, 'end_position': <tf.Tensor 'IteratorGetNext:6' shape=(None,) dtype=int64>, 'peptide_seq': <tf.Tensor 'IteratorGetNext:19' shape=(None,) dtype=string>, 'chou_fasman': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=float32>, 'emini': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=float32>, 'kolaskar_tongaonkar': <tf.Tensor 'IteratorGetNext:12' shape=(None,) dtype=float32>, 'parker': <tf.Tensor 'IteratorGetNext:15' shape=(None,) dtype=float32>, 'isoelectric_point': <tf.Tensor 'IteratorGetNext:10' shape=(None,) dtype=float32>, 'aromaticity': <tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=float32>, 'hydrophobicity': <tf.Tensor 'IteratorGetNext:8' shape=(None,) dtype=float32>, 'stability': <tf.Tensor 'IteratorGetNext:21' shape=(None,) dtype=float32>, 'peptide_length': <tf.Tensor 'IteratorGetNext:17' shape=(None,) dtype=int64>, 'hs_ratio': <tf.Tensor 'IteratorGetNext:7' shape=(None,) dtype=float32>, 'chou_fasman_norm': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=float32>, 'emini_norm': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=float32>, 'kolaskar_tongaonkar_norm': <tf.Tensor 'IteratorGetNext:13' shape=(None,) dtype=float32>, 'parker_norm': <tf.Tensor 'IteratorGetNext:16' shape=(None,) dtype=float32>, 'isoelectric_point_norm': <tf.Tensor 'IteratorGetNext:11' shape=(None,) dtype=float32>, 'aromaticity_norm': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float32>, 'hydrophobicity_norm': <tf.Tensor 'IteratorGetNext:9' shape=(None,) dtype=float32>, 'stability_norm': <tf.Tensor 'IteratorGetNext:22' shape=(None,) dtype=float32>, 'peptide_length_norm': <tf.Tensor 'IteratorGetNext:18' shape=(None,) dtype=float32>, 'start_position_norm': <tf.Tensor 'IteratorGetNext:24' shape=(None,) dtype=float32>}. Consider rewriting this model with the Functional API.\n",
            "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'parent_protein_id': <tf.Tensor 'IteratorGetNext:14' shape=(None,) dtype=string>, 'protein_seq': <tf.Tensor 'IteratorGetNext:20' shape=(None,) dtype=string>, 'start_position': <tf.Tensor 'IteratorGetNext:23' shape=(None,) dtype=int64>, 'end_position': <tf.Tensor 'IteratorGetNext:6' shape=(None,) dtype=int64>, 'peptide_seq': <tf.Tensor 'IteratorGetNext:19' shape=(None,) dtype=string>, 'chou_fasman': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=float32>, 'emini': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=float32>, 'kolaskar_tongaonkar': <tf.Tensor 'IteratorGetNext:12' shape=(None,) dtype=float32>, 'parker': <tf.Tensor 'IteratorGetNext:15' shape=(None,) dtype=float32>, 'isoelectric_point': <tf.Tensor 'IteratorGetNext:10' shape=(None,) dtype=float32>, 'aromaticity': <tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=float32>, 'hydrophobicity': <tf.Tensor 'IteratorGetNext:8' shape=(None,) dtype=float32>, 'stability': <tf.Tensor 'IteratorGetNext:21' shape=(None,) dtype=float32>, 'peptide_length': <tf.Tensor 'IteratorGetNext:17' shape=(None,) dtype=int64>, 'hs_ratio': <tf.Tensor 'IteratorGetNext:7' shape=(None,) dtype=float32>, 'chou_fasman_norm': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=float32>, 'emini_norm': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=float32>, 'kolaskar_tongaonkar_norm': <tf.Tensor 'IteratorGetNext:13' shape=(None,) dtype=float32>, 'parker_norm': <tf.Tensor 'IteratorGetNext:16' shape=(None,) dtype=float32>, 'isoelectric_point_norm': <tf.Tensor 'IteratorGetNext:11' shape=(None,) dtype=float32>, 'aromaticity_norm': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float32>, 'hydrophobicity_norm': <tf.Tensor 'IteratorGetNext:9' shape=(None,) dtype=float32>, 'stability_norm': <tf.Tensor 'IteratorGetNext:22' shape=(None,) dtype=float32>, 'peptide_length_norm': <tf.Tensor 'IteratorGetNext:18' shape=(None,) dtype=float32>, 'start_position_norm': <tf.Tensor 'IteratorGetNext:24' shape=(None,) dtype=float32>}. Consider rewriting this model with the Functional API.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105/105 [==============================] - 2s 4ms/step - loss: 0.6284 - accuracy: 0.2982 - precision: 0.2770 - recall: 0.9852 - auc: 0.5772\n",
            "Epoch 2/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.5462 - precision: 0.3454 - recall: 0.7504 - auc: 0.6518\n",
            "Epoch 3/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.5533 - accuracy: 0.6017 - precision: 0.3716 - recall: 0.6766 - auc: 0.6649\n",
            "Epoch 4/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.5500 - accuracy: 0.6079 - precision: 0.3773 - recall: 0.6836 - auc: 0.6718\n",
            "Epoch 5/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.5474 - accuracy: 0.6165 - precision: 0.3851 - recall: 0.6921 - auc: 0.6786\n",
            "Epoch 6/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.5439 - accuracy: 0.6150 - precision: 0.3859 - recall: 0.7073 - auc: 0.6854\n",
            "Epoch 7/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.5406 - accuracy: 0.6248 - precision: 0.3914 - recall: 0.6889 - auc: 0.6922\n",
            "Epoch 8/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.5368 - accuracy: 0.6133 - precision: 0.3866 - recall: 0.7239 - auc: 0.6998\n",
            "Epoch 9/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.5332 - accuracy: 0.6303 - precision: 0.3974 - recall: 0.7009 - auc: 0.7060\n",
            "Epoch 10/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.5294 - accuracy: 0.6300 - precision: 0.3990 - recall: 0.7179 - auc: 0.7122\n",
            "Epoch 11/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.5265 - accuracy: 0.6346 - precision: 0.4034 - recall: 0.7235 - auc: 0.7178\n",
            "Epoch 12/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.5235 - accuracy: 0.6392 - precision: 0.4071 - recall: 0.7221 - auc: 0.7226\n",
            "Epoch 13/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.5203 - accuracy: 0.6363 - precision: 0.4056 - recall: 0.7309 - auc: 0.7277\n",
            "Epoch 14/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.5166 - accuracy: 0.6491 - precision: 0.4157 - recall: 0.7218 - auc: 0.7335\n",
            "Epoch 15/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.5142 - accuracy: 0.6441 - precision: 0.4126 - recall: 0.7341 - auc: 0.7371\n",
            "Epoch 16/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.5108 - accuracy: 0.6497 - precision: 0.4175 - recall: 0.7348 - auc: 0.7426\n",
            "Epoch 17/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.5079 - accuracy: 0.6496 - precision: 0.4174 - recall: 0.7352 - auc: 0.7463\n",
            "Epoch 18/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.5051 - accuracy: 0.6545 - precision: 0.4217 - recall: 0.7345 - auc: 0.7503\n",
            "Epoch 19/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.5015 - accuracy: 0.6571 - precision: 0.4251 - recall: 0.7472 - auc: 0.7559\n",
            "Epoch 20/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4988 - accuracy: 0.6630 - precision: 0.4296 - recall: 0.7373 - auc: 0.7588\n",
            "Epoch 21/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4961 - accuracy: 0.6742 - precision: 0.4409 - recall: 0.7472 - auc: 0.7630\n",
            "Epoch 22/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4930 - accuracy: 0.6796 - precision: 0.4458 - recall: 0.7419 - auc: 0.7668\n",
            "Epoch 23/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4904 - accuracy: 0.6735 - precision: 0.4401 - recall: 0.7454 - auc: 0.7692\n",
            "Epoch 24/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4873 - accuracy: 0.6905 - precision: 0.4564 - recall: 0.7341 - auc: 0.7737\n",
            "Epoch 25/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4854 - accuracy: 0.6828 - precision: 0.4491 - recall: 0.7454 - auc: 0.7754\n",
            "Epoch 26/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4824 - accuracy: 0.6937 - precision: 0.4600 - recall: 0.7391 - auc: 0.7790\n",
            "Epoch 27/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4805 - accuracy: 0.7027 - precision: 0.4697 - recall: 0.7401 - auc: 0.7810\n",
            "Epoch 28/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4782 - accuracy: 0.6975 - precision: 0.4642 - recall: 0.7426 - auc: 0.7842\n",
            "Epoch 29/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4757 - accuracy: 0.7062 - precision: 0.4738 - recall: 0.7479 - auc: 0.7872\n",
            "Epoch 30/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4737 - accuracy: 0.7114 - precision: 0.4795 - recall: 0.7383 - auc: 0.7895\n",
            "Epoch 31/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4713 - accuracy: 0.7111 - precision: 0.4794 - recall: 0.7482 - auc: 0.7920\n",
            "Epoch 32/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4693 - accuracy: 0.7177 - precision: 0.4869 - recall: 0.7486 - auc: 0.7939\n",
            "Epoch 33/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4672 - accuracy: 0.7171 - precision: 0.4862 - recall: 0.7440 - auc: 0.7964\n",
            "Epoch 34/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4652 - accuracy: 0.7193 - precision: 0.4888 - recall: 0.7475 - auc: 0.7991\n",
            "Epoch 35/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4633 - accuracy: 0.7183 - precision: 0.4878 - recall: 0.7542 - auc: 0.8007\n",
            "Epoch 36/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4617 - accuracy: 0.7212 - precision: 0.4912 - recall: 0.7560 - auc: 0.8025\n",
            "Epoch 37/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4605 - accuracy: 0.7281 - precision: 0.4994 - recall: 0.7486 - auc: 0.8036\n",
            "Epoch 38/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4582 - accuracy: 0.7306 - precision: 0.5025 - recall: 0.7574 - auc: 0.8058\n",
            "Epoch 39/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4579 - accuracy: 0.7222 - precision: 0.4923 - recall: 0.7592 - auc: 0.8060\n",
            "Epoch 40/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4564 - accuracy: 0.7245 - precision: 0.4951 - recall: 0.7620 - auc: 0.8079\n",
            "Epoch 41/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4541 - accuracy: 0.7299 - precision: 0.5016 - recall: 0.7553 - auc: 0.8097\n",
            "Epoch 42/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4531 - accuracy: 0.7325 - precision: 0.5048 - recall: 0.7609 - auc: 0.8116\n",
            "Epoch 43/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4512 - accuracy: 0.7323 - precision: 0.5046 - recall: 0.7631 - auc: 0.8136\n",
            "Epoch 44/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4494 - accuracy: 0.7328 - precision: 0.5051 - recall: 0.7638 - auc: 0.8149\n",
            "Epoch 45/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4488 - accuracy: 0.7382 - precision: 0.5119 - recall: 0.7567 - auc: 0.8161\n",
            "Epoch 46/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4479 - accuracy: 0.7357 - precision: 0.5086 - recall: 0.7722 - auc: 0.8166\n",
            "Epoch 47/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4465 - accuracy: 0.7410 - precision: 0.5156 - recall: 0.7599 - auc: 0.8185\n",
            "Epoch 48/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4449 - accuracy: 0.7405 - precision: 0.5146 - recall: 0.7708 - auc: 0.8199\n",
            "Epoch 49/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4437 - accuracy: 0.7398 - precision: 0.5139 - recall: 0.7627 - auc: 0.8208\n",
            "Epoch 50/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4430 - accuracy: 0.7408 - precision: 0.5151 - recall: 0.7631 - auc: 0.8220\n",
            "Epoch 51/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4411 - accuracy: 0.7443 - precision: 0.5196 - recall: 0.7666 - auc: 0.8237\n",
            "Epoch 52/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4400 - accuracy: 0.7398 - precision: 0.5138 - recall: 0.7712 - auc: 0.8247\n",
            "Epoch 53/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4391 - accuracy: 0.7495 - precision: 0.5264 - recall: 0.7680 - auc: 0.8257\n",
            "Epoch 54/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4387 - accuracy: 0.7455 - precision: 0.5211 - recall: 0.7712 - auc: 0.8264\n",
            "Epoch 55/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4373 - accuracy: 0.7502 - precision: 0.5271 - recall: 0.7751 - auc: 0.8273\n",
            "Epoch 56/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4358 - accuracy: 0.7491 - precision: 0.5258 - recall: 0.7712 - auc: 0.8289\n",
            "Epoch 57/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4342 - accuracy: 0.7527 - precision: 0.5304 - recall: 0.7754 - auc: 0.8307\n",
            "Epoch 58/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4348 - accuracy: 0.7490 - precision: 0.5255 - recall: 0.7744 - auc: 0.8306\n",
            "Epoch 59/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4325 - accuracy: 0.7524 - precision: 0.5298 - recall: 0.7814 - auc: 0.8324\n",
            "Epoch 60/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4317 - accuracy: 0.7556 - precision: 0.5341 - recall: 0.7807 - auc: 0.8336\n",
            "Epoch 61/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4306 - accuracy: 0.7537 - precision: 0.5318 - recall: 0.7744 - auc: 0.8341\n",
            "Epoch 62/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4296 - accuracy: 0.7522 - precision: 0.5294 - recall: 0.7832 - auc: 0.8351\n",
            "Epoch 63/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4295 - accuracy: 0.7531 - precision: 0.5308 - recall: 0.7797 - auc: 0.8356\n",
            "Epoch 64/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4276 - accuracy: 0.7550 - precision: 0.5330 - recall: 0.7864 - auc: 0.8371\n",
            "Epoch 65/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4270 - accuracy: 0.7556 - precision: 0.5338 - recall: 0.7857 - auc: 0.8377\n",
            "Epoch 66/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4256 - accuracy: 0.7594 - precision: 0.5389 - recall: 0.7881 - auc: 0.8391\n",
            "Epoch 67/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4254 - accuracy: 0.7588 - precision: 0.5381 - recall: 0.7857 - auc: 0.8391\n",
            "Epoch 68/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4245 - accuracy: 0.7574 - precision: 0.5363 - recall: 0.7857 - auc: 0.8398\n",
            "Epoch 69/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4233 - accuracy: 0.7607 - precision: 0.5408 - recall: 0.7832 - auc: 0.8414\n",
            "Epoch 70/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4218 - accuracy: 0.7566 - precision: 0.5349 - recall: 0.7910 - auc: 0.8425\n",
            "Epoch 71/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4213 - accuracy: 0.7605 - precision: 0.5401 - recall: 0.7917 - auc: 0.8432\n",
            "Epoch 72/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4205 - accuracy: 0.7645 - precision: 0.5460 - recall: 0.7857 - auc: 0.8439\n",
            "Epoch 73/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4196 - accuracy: 0.7636 - precision: 0.5442 - recall: 0.7938 - auc: 0.8449\n",
            "Epoch 74/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4187 - accuracy: 0.7630 - precision: 0.5436 - recall: 0.7910 - auc: 0.8455\n",
            "Epoch 75/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4184 - accuracy: 0.7589 - precision: 0.5376 - recall: 0.7970 - auc: 0.8462\n",
            "Epoch 76/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4171 - accuracy: 0.7668 - precision: 0.5491 - recall: 0.7878 - auc: 0.8468\n",
            "Epoch 77/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4163 - accuracy: 0.7670 - precision: 0.5490 - recall: 0.7934 - auc: 0.8477\n",
            "Epoch 78/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4154 - accuracy: 0.7647 - precision: 0.5452 - recall: 0.8030 - auc: 0.8488\n",
            "Epoch 79/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4139 - accuracy: 0.7686 - precision: 0.5512 - recall: 0.7945 - auc: 0.8499\n",
            "Epoch 80/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4139 - accuracy: 0.7682 - precision: 0.5503 - recall: 0.7984 - auc: 0.8495\n",
            "Epoch 81/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4130 - accuracy: 0.7690 - precision: 0.5513 - recall: 0.8005 - auc: 0.8512\n",
            "Epoch 82/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4118 - accuracy: 0.7707 - precision: 0.5535 - recall: 0.8016 - auc: 0.8516\n",
            "Epoch 83/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4110 - accuracy: 0.7697 - precision: 0.5521 - recall: 0.8023 - auc: 0.8523\n",
            "Epoch 84/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4098 - accuracy: 0.7748 - precision: 0.5592 - recall: 0.8040 - auc: 0.8535\n",
            "Epoch 85/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4088 - accuracy: 0.7744 - precision: 0.5588 - recall: 0.8016 - auc: 0.8541\n",
            "Epoch 86/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4092 - accuracy: 0.7745 - precision: 0.5589 - recall: 0.8026 - auc: 0.8542\n",
            "Epoch 87/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4078 - accuracy: 0.7729 - precision: 0.5569 - recall: 0.7980 - auc: 0.8548\n",
            "Epoch 88/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4065 - accuracy: 0.7728 - precision: 0.5564 - recall: 0.8030 - auc: 0.8561\n",
            "Epoch 89/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4066 - accuracy: 0.7763 - precision: 0.5609 - recall: 0.8097 - auc: 0.8564\n",
            "Epoch 90/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4054 - accuracy: 0.7756 - precision: 0.5604 - recall: 0.8047 - auc: 0.8573\n",
            "Epoch 91/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4040 - accuracy: 0.7758 - precision: 0.5610 - recall: 0.8001 - auc: 0.8582\n",
            "Epoch 92/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4035 - accuracy: 0.7800 - precision: 0.5666 - recall: 0.8051 - auc: 0.8586\n",
            "Epoch 93/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4029 - accuracy: 0.7781 - precision: 0.5637 - recall: 0.8076 - auc: 0.8592\n",
            "Epoch 94/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4023 - accuracy: 0.7811 - precision: 0.5681 - recall: 0.8069 - auc: 0.8600\n",
            "Epoch 95/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.4007 - accuracy: 0.7776 - precision: 0.5626 - recall: 0.8107 - auc: 0.8611\n",
            "Epoch 96/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.4005 - accuracy: 0.7811 - precision: 0.5682 - recall: 0.8058 - auc: 0.8613\n",
            "Epoch 97/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3986 - accuracy: 0.7813 - precision: 0.5680 - recall: 0.8107 - auc: 0.8630\n",
            "Epoch 98/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3975 - accuracy: 0.7839 - precision: 0.5715 - recall: 0.8139 - auc: 0.8640\n",
            "Epoch 99/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3979 - accuracy: 0.7820 - precision: 0.5690 - recall: 0.8114 - auc: 0.8631\n",
            "Epoch 100/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3966 - accuracy: 0.7819 - precision: 0.5686 - recall: 0.8132 - auc: 0.8643\n",
            "Epoch 101/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3959 - accuracy: 0.7827 - precision: 0.5704 - recall: 0.8086 - auc: 0.8650\n",
            "Epoch 102/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3954 - accuracy: 0.7826 - precision: 0.5696 - recall: 0.8153 - auc: 0.8650\n",
            "Epoch 103/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3944 - accuracy: 0.7849 - precision: 0.5731 - recall: 0.8139 - auc: 0.8660\n",
            "Epoch 104/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3939 - accuracy: 0.7853 - precision: 0.5732 - recall: 0.8185 - auc: 0.8666\n",
            "Epoch 105/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3935 - accuracy: 0.7832 - precision: 0.5702 - recall: 0.8174 - auc: 0.8667\n",
            "Epoch 106/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3930 - accuracy: 0.7892 - precision: 0.5796 - recall: 0.8121 - auc: 0.8671\n",
            "Epoch 107/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3919 - accuracy: 0.7850 - precision: 0.5728 - recall: 0.8181 - auc: 0.8683\n",
            "Epoch 108/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3909 - accuracy: 0.7855 - precision: 0.5741 - recall: 0.8125 - auc: 0.8690\n",
            "Epoch 109/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3908 - accuracy: 0.7876 - precision: 0.5762 - recall: 0.8224 - auc: 0.8689\n",
            "Epoch 110/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3896 - accuracy: 0.7918 - precision: 0.5834 - recall: 0.8150 - auc: 0.8700\n",
            "Epoch 111/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3887 - accuracy: 0.7874 - precision: 0.5761 - recall: 0.8210 - auc: 0.8710\n",
            "Epoch 112/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3872 - accuracy: 0.7892 - precision: 0.5786 - recall: 0.8227 - auc: 0.8720\n",
            "Epoch 113/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3868 - accuracy: 0.7911 - precision: 0.5820 - recall: 0.8174 - auc: 0.8721\n",
            "Epoch 114/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3863 - accuracy: 0.7904 - precision: 0.5801 - recall: 0.8245 - auc: 0.8724\n",
            "Epoch 115/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3859 - accuracy: 0.7902 - precision: 0.5805 - recall: 0.8185 - auc: 0.8727\n",
            "Epoch 116/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3856 - accuracy: 0.7905 - precision: 0.5810 - recall: 0.8181 - auc: 0.8727\n",
            "Epoch 117/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3840 - accuracy: 0.7915 - precision: 0.5825 - recall: 0.8178 - auc: 0.8743\n",
            "Epoch 118/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3829 - accuracy: 0.7929 - precision: 0.5842 - recall: 0.8220 - auc: 0.8753\n",
            "Epoch 119/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3824 - accuracy: 0.7926 - precision: 0.5838 - recall: 0.8213 - auc: 0.8756\n",
            "Epoch 120/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3824 - accuracy: 0.7898 - precision: 0.5799 - recall: 0.8192 - auc: 0.8758\n",
            "Epoch 121/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3805 - accuracy: 0.7937 - precision: 0.5851 - recall: 0.8245 - auc: 0.8765\n",
            "Epoch 122/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3801 - accuracy: 0.7915 - precision: 0.5818 - recall: 0.8238 - auc: 0.8776\n",
            "Epoch 123/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3797 - accuracy: 0.7939 - precision: 0.5860 - recall: 0.8203 - auc: 0.8774\n",
            "Epoch 124/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3797 - accuracy: 0.7921 - precision: 0.5826 - recall: 0.8252 - auc: 0.8776\n",
            "Epoch 125/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3780 - accuracy: 0.7952 - precision: 0.5876 - recall: 0.8227 - auc: 0.8786\n",
            "Epoch 126/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3773 - accuracy: 0.7939 - precision: 0.5856 - recall: 0.8238 - auc: 0.8793\n",
            "Epoch 127/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3783 - accuracy: 0.7941 - precision: 0.5865 - recall: 0.8189 - auc: 0.8784\n",
            "Epoch 128/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3767 - accuracy: 0.7984 - precision: 0.5926 - recall: 0.8238 - auc: 0.8801\n",
            "Epoch 129/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3755 - accuracy: 0.7967 - precision: 0.5898 - recall: 0.8245 - auc: 0.8805\n",
            "Epoch 130/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3741 - accuracy: 0.7953 - precision: 0.5878 - recall: 0.8227 - auc: 0.8814\n",
            "Epoch 131/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3751 - accuracy: 0.7959 - precision: 0.5878 - recall: 0.8298 - auc: 0.8808\n",
            "Epoch 132/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3732 - accuracy: 0.7984 - precision: 0.5913 - recall: 0.8323 - auc: 0.8825\n",
            "Epoch 133/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3728 - accuracy: 0.7994 - precision: 0.5942 - recall: 0.8231 - auc: 0.8824\n",
            "Epoch 134/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3726 - accuracy: 0.7995 - precision: 0.5933 - recall: 0.8309 - auc: 0.8827\n",
            "Epoch 135/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3715 - accuracy: 0.7991 - precision: 0.5926 - recall: 0.8312 - auc: 0.8835\n",
            "Epoch 136/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3714 - accuracy: 0.7990 - precision: 0.5935 - recall: 0.8238 - auc: 0.8834\n",
            "Epoch 137/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3718 - accuracy: 0.7970 - precision: 0.5896 - recall: 0.8298 - auc: 0.8831\n",
            "Epoch 138/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3700 - accuracy: 0.8008 - precision: 0.5955 - recall: 0.8298 - auc: 0.8850\n",
            "Epoch 139/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3699 - accuracy: 0.7965 - precision: 0.5884 - recall: 0.8330 - auc: 0.8846\n",
            "Epoch 140/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3690 - accuracy: 0.7996 - precision: 0.5942 - recall: 0.8256 - auc: 0.8851\n",
            "Epoch 141/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3689 - accuracy: 0.7983 - precision: 0.5919 - recall: 0.8270 - auc: 0.8852\n",
            "Epoch 142/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3684 - accuracy: 0.8010 - precision: 0.5963 - recall: 0.8263 - auc: 0.8856\n",
            "Epoch 143/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3680 - accuracy: 0.8031 - precision: 0.5985 - recall: 0.8344 - auc: 0.8859\n",
            "Epoch 144/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3671 - accuracy: 0.7988 - precision: 0.5922 - recall: 0.8309 - auc: 0.8867\n",
            "Epoch 145/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3659 - accuracy: 0.8043 - precision: 0.6014 - recall: 0.8270 - auc: 0.8873\n",
            "Epoch 146/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3654 - accuracy: 0.8030 - precision: 0.5986 - recall: 0.8316 - auc: 0.8876\n",
            "Epoch 147/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3662 - accuracy: 0.8021 - precision: 0.5988 - recall: 0.8206 - auc: 0.8871\n",
            "Epoch 148/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3655 - accuracy: 0.8007 - precision: 0.5954 - recall: 0.8298 - auc: 0.8873\n",
            "Epoch 149/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3641 - accuracy: 0.8057 - precision: 0.6028 - recall: 0.8333 - auc: 0.8881\n",
            "Epoch 150/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3635 - accuracy: 0.8037 - precision: 0.5996 - recall: 0.8330 - auc: 0.8890\n",
            "Epoch 151/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3635 - accuracy: 0.8025 - precision: 0.5977 - recall: 0.8330 - auc: 0.8891\n",
            "Epoch 152/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3628 - accuracy: 0.8061 - precision: 0.6042 - recall: 0.8280 - auc: 0.8899\n",
            "Epoch 153/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3627 - accuracy: 0.8009 - precision: 0.5963 - recall: 0.8256 - auc: 0.8893\n",
            "Epoch 154/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3627 - accuracy: 0.8036 - precision: 0.6005 - recall: 0.8259 - auc: 0.8893\n",
            "Epoch 155/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3611 - accuracy: 0.8069 - precision: 0.6053 - recall: 0.8294 - auc: 0.8909\n",
            "Epoch 156/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3611 - accuracy: 0.8075 - precision: 0.6065 - recall: 0.8277 - auc: 0.8908\n",
            "Epoch 157/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3609 - accuracy: 0.8065 - precision: 0.6039 - recall: 0.8344 - auc: 0.8904\n",
            "Epoch 158/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3610 - accuracy: 0.8089 - precision: 0.6086 - recall: 0.8291 - auc: 0.8910\n",
            "Epoch 159/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3593 - accuracy: 0.8055 - precision: 0.6032 - recall: 0.8287 - auc: 0.8914\n",
            "Epoch 160/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3591 - accuracy: 0.8059 - precision: 0.6034 - recall: 0.8312 - auc: 0.8919\n",
            "Epoch 161/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3596 - accuracy: 0.8053 - precision: 0.6027 - recall: 0.8287 - auc: 0.8914\n",
            "Epoch 162/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3584 - accuracy: 0.8039 - precision: 0.5996 - recall: 0.8355 - auc: 0.8923\n",
            "Epoch 163/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3584 - accuracy: 0.8096 - precision: 0.6095 - recall: 0.8302 - auc: 0.8924\n",
            "Epoch 164/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3574 - accuracy: 0.8075 - precision: 0.6062 - recall: 0.8294 - auc: 0.8929\n",
            "Epoch 165/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3564 - accuracy: 0.8085 - precision: 0.6073 - recall: 0.8333 - auc: 0.8934\n",
            "Epoch 166/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3565 - accuracy: 0.8082 - precision: 0.6071 - recall: 0.8319 - auc: 0.8939\n",
            "Epoch 167/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3558 - accuracy: 0.8083 - precision: 0.6073 - recall: 0.8312 - auc: 0.8939\n",
            "Epoch 168/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3561 - accuracy: 0.8077 - precision: 0.6057 - recall: 0.8358 - auc: 0.8937\n",
            "Epoch 169/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3550 - accuracy: 0.8087 - precision: 0.6073 - recall: 0.8351 - auc: 0.8945\n",
            "Epoch 170/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3545 - accuracy: 0.8079 - precision: 0.6067 - recall: 0.8312 - auc: 0.8948\n",
            "Epoch 171/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3552 - accuracy: 0.8104 - precision: 0.6106 - recall: 0.8326 - auc: 0.8941\n",
            "Epoch 172/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3540 - accuracy: 0.8106 - precision: 0.6105 - recall: 0.8347 - auc: 0.8953\n",
            "Epoch 173/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3545 - accuracy: 0.8112 - precision: 0.6112 - recall: 0.8365 - auc: 0.8948\n",
            "Epoch 174/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3538 - accuracy: 0.8079 - precision: 0.6064 - recall: 0.8330 - auc: 0.8950\n",
            "Epoch 175/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3531 - accuracy: 0.8113 - precision: 0.6118 - recall: 0.8340 - auc: 0.8958\n",
            "Epoch 176/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3526 - accuracy: 0.8139 - precision: 0.6169 - recall: 0.8294 - auc: 0.8959\n",
            "Epoch 177/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3531 - accuracy: 0.8080 - precision: 0.6056 - recall: 0.8393 - auc: 0.8958\n",
            "Epoch 178/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3536 - accuracy: 0.8113 - precision: 0.6119 - recall: 0.8330 - auc: 0.8955\n",
            "Epoch 179/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3513 - accuracy: 0.8094 - precision: 0.6084 - recall: 0.8351 - auc: 0.8969\n",
            "Epoch 180/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3511 - accuracy: 0.8142 - precision: 0.6161 - recall: 0.8365 - auc: 0.8968\n",
            "Epoch 181/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3504 - accuracy: 0.8123 - precision: 0.6130 - recall: 0.8369 - auc: 0.8975\n",
            "Epoch 182/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3509 - accuracy: 0.8146 - precision: 0.6175 - recall: 0.8333 - auc: 0.8969\n",
            "Epoch 183/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3522 - accuracy: 0.8109 - precision: 0.6108 - recall: 0.8358 - auc: 0.8961\n",
            "Epoch 184/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3505 - accuracy: 0.8135 - precision: 0.6153 - recall: 0.8347 - auc: 0.8973\n",
            "Epoch 185/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3494 - accuracy: 0.8125 - precision: 0.6131 - recall: 0.8383 - auc: 0.8981\n",
            "Epoch 186/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3498 - accuracy: 0.8130 - precision: 0.6151 - recall: 0.8312 - auc: 0.8976\n",
            "Epoch 187/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3489 - accuracy: 0.8126 - precision: 0.6132 - recall: 0.8390 - auc: 0.8985\n",
            "Epoch 188/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3512 - accuracy: 0.8139 - precision: 0.6157 - recall: 0.8362 - auc: 0.8965\n",
            "Epoch 189/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3489 - accuracy: 0.8129 - precision: 0.6135 - recall: 0.8400 - auc: 0.8983\n",
            "Epoch 190/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3478 - accuracy: 0.8129 - precision: 0.6146 - recall: 0.8330 - auc: 0.8991\n",
            "Epoch 191/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3481 - accuracy: 0.8153 - precision: 0.6182 - recall: 0.8355 - auc: 0.8985\n",
            "Epoch 192/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3479 - accuracy: 0.8139 - precision: 0.6156 - recall: 0.8365 - auc: 0.8989\n",
            "Epoch 193/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3470 - accuracy: 0.8141 - precision: 0.6150 - recall: 0.8422 - auc: 0.8994\n",
            "Epoch 194/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3478 - accuracy: 0.8169 - precision: 0.6202 - recall: 0.8397 - auc: 0.8990\n",
            "Epoch 195/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3467 - accuracy: 0.8144 - precision: 0.6161 - recall: 0.8386 - auc: 0.8997\n",
            "Epoch 196/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3465 - accuracy: 0.8145 - precision: 0.6162 - recall: 0.8397 - auc: 0.8997\n",
            "Epoch 197/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3480 - accuracy: 0.8142 - precision: 0.6167 - recall: 0.8330 - auc: 0.8985\n",
            "Epoch 198/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3453 - accuracy: 0.8146 - precision: 0.6163 - recall: 0.8400 - auc: 0.9002\n",
            "Epoch 199/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3466 - accuracy: 0.8138 - precision: 0.6158 - recall: 0.8344 - auc: 0.8992\n",
            "Epoch 200/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3462 - accuracy: 0.8150 - precision: 0.6170 - recall: 0.8397 - auc: 0.9000\n",
            "Epoch 201/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3448 - accuracy: 0.8175 - precision: 0.6213 - recall: 0.8390 - auc: 0.9008\n",
            "Epoch 202/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3450 - accuracy: 0.8119 - precision: 0.6117 - recall: 0.8404 - auc: 0.9007\n",
            "Epoch 203/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3439 - accuracy: 0.8170 - precision: 0.6203 - recall: 0.8400 - auc: 0.9015\n",
            "Epoch 204/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3450 - accuracy: 0.8152 - precision: 0.6173 - recall: 0.8397 - auc: 0.9006\n",
            "Epoch 205/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3439 - accuracy: 0.8148 - precision: 0.6167 - recall: 0.8397 - auc: 0.9013\n",
            "Epoch 206/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3432 - accuracy: 0.8163 - precision: 0.6194 - recall: 0.8383 - auc: 0.9020\n",
            "Epoch 207/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3434 - accuracy: 0.8155 - precision: 0.6173 - recall: 0.8429 - auc: 0.9017\n",
            "Epoch 208/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3434 - accuracy: 0.8182 - precision: 0.6216 - recall: 0.8436 - auc: 0.9017\n",
            "Epoch 209/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3428 - accuracy: 0.8178 - precision: 0.6208 - recall: 0.8450 - auc: 0.9015\n",
            "Epoch 210/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3430 - accuracy: 0.8175 - precision: 0.6200 - recall: 0.8464 - auc: 0.9021\n",
            "Epoch 211/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3418 - accuracy: 0.8145 - precision: 0.6162 - recall: 0.8400 - auc: 0.9025\n",
            "Epoch 212/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3420 - accuracy: 0.8167 - precision: 0.6196 - recall: 0.8407 - auc: 0.9024\n",
            "Epoch 213/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3424 - accuracy: 0.8163 - precision: 0.6181 - recall: 0.8453 - auc: 0.9019\n",
            "Epoch 214/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3412 - accuracy: 0.8170 - precision: 0.6205 - recall: 0.8393 - auc: 0.9030\n",
            "Epoch 215/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3419 - accuracy: 0.8154 - precision: 0.6175 - recall: 0.8404 - auc: 0.9027\n",
            "Epoch 216/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3411 - accuracy: 0.8179 - precision: 0.6217 - recall: 0.8404 - auc: 0.9034\n",
            "Epoch 217/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3409 - accuracy: 0.8193 - precision: 0.6235 - recall: 0.8439 - auc: 0.9034\n",
            "Epoch 218/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3408 - accuracy: 0.8191 - precision: 0.6230 - recall: 0.8443 - auc: 0.9034\n",
            "Epoch 219/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3401 - accuracy: 0.8183 - precision: 0.6221 - recall: 0.8418 - auc: 0.9038\n",
            "Epoch 220/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3401 - accuracy: 0.8169 - precision: 0.6192 - recall: 0.8453 - auc: 0.9038\n",
            "Epoch 221/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3399 - accuracy: 0.8182 - precision: 0.6218 - recall: 0.8425 - auc: 0.9041\n",
            "Epoch 222/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3389 - accuracy: 0.8179 - precision: 0.6215 - recall: 0.8415 - auc: 0.9041\n",
            "Epoch 223/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3403 - accuracy: 0.8183 - precision: 0.6223 - recall: 0.8411 - auc: 0.9038\n",
            "Epoch 224/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3387 - accuracy: 0.8195 - precision: 0.6232 - recall: 0.8475 - auc: 0.9047\n",
            "Epoch 225/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3385 - accuracy: 0.8220 - precision: 0.6281 - recall: 0.8443 - auc: 0.9046\n",
            "Epoch 226/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3377 - accuracy: 0.8184 - precision: 0.6214 - recall: 0.8468 - auc: 0.9053\n",
            "Epoch 227/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3385 - accuracy: 0.8192 - precision: 0.6236 - recall: 0.8425 - auc: 0.9049\n",
            "Epoch 228/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3387 - accuracy: 0.8188 - precision: 0.6221 - recall: 0.8464 - auc: 0.9044\n",
            "Epoch 229/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3385 - accuracy: 0.8196 - precision: 0.6240 - recall: 0.8439 - auc: 0.9045\n",
            "Epoch 230/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3372 - accuracy: 0.8196 - precision: 0.6240 - recall: 0.8439 - auc: 0.9056\n",
            "Epoch 231/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3372 - accuracy: 0.8207 - precision: 0.6256 - recall: 0.8450 - auc: 0.9057\n",
            "Epoch 232/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3376 - accuracy: 0.8185 - precision: 0.6222 - recall: 0.8432 - auc: 0.9054\n",
            "Epoch 233/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3373 - accuracy: 0.8205 - precision: 0.6243 - recall: 0.8503 - auc: 0.9053\n",
            "Epoch 234/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3368 - accuracy: 0.8224 - precision: 0.6297 - recall: 0.8393 - auc: 0.9060\n",
            "Epoch 235/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3363 - accuracy: 0.8212 - precision: 0.6259 - recall: 0.8478 - auc: 0.9063\n",
            "Epoch 236/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3364 - accuracy: 0.8227 - precision: 0.6290 - recall: 0.8453 - auc: 0.9060\n",
            "Epoch 237/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3359 - accuracy: 0.8170 - precision: 0.6197 - recall: 0.8439 - auc: 0.9062\n",
            "Epoch 238/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3369 - accuracy: 0.8209 - precision: 0.6258 - recall: 0.8457 - auc: 0.9057\n",
            "Epoch 239/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3353 - accuracy: 0.8225 - precision: 0.6291 - recall: 0.8432 - auc: 0.9070\n",
            "Epoch 240/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3346 - accuracy: 0.8235 - precision: 0.6301 - recall: 0.8468 - auc: 0.9071\n",
            "Epoch 241/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3349 - accuracy: 0.8214 - precision: 0.6254 - recall: 0.8524 - auc: 0.9069\n",
            "Epoch 242/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3339 - accuracy: 0.8220 - precision: 0.6277 - recall: 0.8464 - auc: 0.9079\n",
            "Epoch 243/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3347 - accuracy: 0.8204 - precision: 0.6254 - recall: 0.8436 - auc: 0.9069\n",
            "Epoch 244/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3345 - accuracy: 0.8190 - precision: 0.6229 - recall: 0.8436 - auc: 0.9073\n",
            "Epoch 245/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3348 - accuracy: 0.8213 - precision: 0.6271 - recall: 0.8425 - auc: 0.9068\n",
            "Epoch 246/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3330 - accuracy: 0.8217 - precision: 0.6266 - recall: 0.8492 - auc: 0.9081\n",
            "Epoch 247/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3336 - accuracy: 0.8222 - precision: 0.6279 - recall: 0.8468 - auc: 0.9076\n",
            "Epoch 248/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3334 - accuracy: 0.8227 - precision: 0.6284 - recall: 0.8485 - auc: 0.9080\n",
            "Epoch 249/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3335 - accuracy: 0.8195 - precision: 0.6227 - recall: 0.8503 - auc: 0.9079\n",
            "Epoch 250/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3320 - accuracy: 0.8251 - precision: 0.6327 - recall: 0.8475 - auc: 0.9086\n",
            "Epoch 251/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3324 - accuracy: 0.8213 - precision: 0.6260 - recall: 0.8482 - auc: 0.9085\n",
            "Epoch 252/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3320 - accuracy: 0.8226 - precision: 0.6281 - recall: 0.8492 - auc: 0.9088\n",
            "Epoch 253/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3331 - accuracy: 0.8233 - precision: 0.6290 - recall: 0.8503 - auc: 0.9080\n",
            "Epoch 254/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3322 - accuracy: 0.8237 - precision: 0.6310 - recall: 0.8443 - auc: 0.9084\n",
            "Epoch 255/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3317 - accuracy: 0.8215 - precision: 0.6261 - recall: 0.8503 - auc: 0.9089\n",
            "Epoch 256/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3316 - accuracy: 0.8251 - precision: 0.6324 - recall: 0.8492 - auc: 0.9087\n",
            "Epoch 257/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3307 - accuracy: 0.8257 - precision: 0.6333 - recall: 0.8496 - auc: 0.9094\n",
            "Epoch 258/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3316 - accuracy: 0.8239 - precision: 0.6299 - recall: 0.8517 - auc: 0.9088\n",
            "Epoch 259/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3314 - accuracy: 0.8207 - precision: 0.6252 - recall: 0.8475 - auc: 0.9087\n",
            "Epoch 260/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3313 - accuracy: 0.8232 - precision: 0.6288 - recall: 0.8510 - auc: 0.9090\n",
            "Epoch 261/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3289 - accuracy: 0.8229 - precision: 0.6276 - recall: 0.8545 - auc: 0.9106\n",
            "Epoch 262/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3302 - accuracy: 0.8256 - precision: 0.6341 - recall: 0.8450 - auc: 0.9095\n",
            "Epoch 263/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3299 - accuracy: 0.8229 - precision: 0.6279 - recall: 0.8531 - auc: 0.9101\n",
            "Epoch 264/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3303 - accuracy: 0.8269 - precision: 0.6359 - recall: 0.8478 - auc: 0.9097\n",
            "Epoch 265/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3293 - accuracy: 0.8267 - precision: 0.6340 - recall: 0.8552 - auc: 0.9103\n",
            "Epoch 266/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3299 - accuracy: 0.8224 - precision: 0.6284 - recall: 0.8460 - auc: 0.9097\n",
            "Epoch 267/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3283 - accuracy: 0.8248 - precision: 0.6306 - recall: 0.8559 - auc: 0.9106\n",
            "Epoch 268/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3293 - accuracy: 0.8276 - precision: 0.6366 - recall: 0.8499 - auc: 0.9104\n",
            "Epoch 269/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3278 - accuracy: 0.8266 - precision: 0.6336 - recall: 0.8566 - auc: 0.9113\n",
            "Epoch 270/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3296 - accuracy: 0.8231 - precision: 0.6288 - recall: 0.8499 - auc: 0.9100\n",
            "Epoch 271/400\n",
            "105/105 [==============================] - 1s 5ms/step - loss: 0.3289 - accuracy: 0.8266 - precision: 0.6354 - recall: 0.8478 - auc: 0.9103\n",
            "Epoch 272/400\n",
            "105/105 [==============================] - 1s 6ms/step - loss: 0.3283 - accuracy: 0.8259 - precision: 0.6337 - recall: 0.8496 - auc: 0.9108\n",
            "Epoch 273/400\n",
            "105/105 [==============================] - 1s 5ms/step - loss: 0.3279 - accuracy: 0.8286 - precision: 0.6385 - recall: 0.8496 - auc: 0.9113\n",
            "Epoch 274/400\n",
            "105/105 [==============================] - 1s 5ms/step - loss: 0.3282 - accuracy: 0.8247 - precision: 0.6304 - recall: 0.8559 - auc: 0.9107\n",
            "Epoch 275/400\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 0.3278 - accuracy: 0.8268 - precision: 0.6349 - recall: 0.8517 - auc: 0.9112\n",
            "Epoch 276/400\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 0.3277 - accuracy: 0.8252 - precision: 0.6310 - recall: 0.8570 - auc: 0.9110\n",
            "Epoch 277/400\n",
            "105/105 [==============================] - 1s 6ms/step - loss: 0.3273 - accuracy: 0.8282 - precision: 0.6378 - recall: 0.8492 - auc: 0.9114\n",
            "Epoch 278/400\n",
            "105/105 [==============================] - 1s 6ms/step - loss: 0.3275 - accuracy: 0.8253 - precision: 0.6326 - recall: 0.8499 - auc: 0.9111\n",
            "Epoch 279/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3262 - accuracy: 0.8254 - precision: 0.6322 - recall: 0.8528 - auc: 0.9119\n",
            "Epoch 280/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3261 - accuracy: 0.8288 - precision: 0.6380 - recall: 0.8538 - auc: 0.9119\n",
            "Epoch 281/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3267 - accuracy: 0.8254 - precision: 0.6321 - recall: 0.8531 - auc: 0.9118\n",
            "Epoch 282/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3263 - accuracy: 0.8278 - precision: 0.6366 - recall: 0.8517 - auc: 0.9120\n",
            "Epoch 283/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3269 - accuracy: 0.8246 - precision: 0.6311 - recall: 0.8517 - auc: 0.9116\n",
            "Epoch 284/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3256 - accuracy: 0.8279 - precision: 0.6369 - recall: 0.8510 - auc: 0.9122\n",
            "Epoch 285/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3265 - accuracy: 0.8263 - precision: 0.6335 - recall: 0.8545 - auc: 0.9116\n",
            "Epoch 286/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3259 - accuracy: 0.8256 - precision: 0.6320 - recall: 0.8556 - auc: 0.9120\n",
            "Epoch 287/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3258 - accuracy: 0.8278 - precision: 0.6372 - recall: 0.8489 - auc: 0.9118\n",
            "Epoch 288/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3261 - accuracy: 0.8272 - precision: 0.6357 - recall: 0.8510 - auc: 0.9117\n",
            "Epoch 289/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3259 - accuracy: 0.8285 - precision: 0.6384 - recall: 0.8492 - auc: 0.9122\n",
            "Epoch 290/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3258 - accuracy: 0.8260 - precision: 0.6340 - recall: 0.8496 - auc: 0.9121\n",
            "Epoch 291/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3257 - accuracy: 0.8276 - precision: 0.6370 - recall: 0.8482 - auc: 0.9121\n",
            "Epoch 292/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3250 - accuracy: 0.8283 - precision: 0.6369 - recall: 0.8552 - auc: 0.9126\n",
            "Epoch 293/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3246 - accuracy: 0.8272 - precision: 0.6352 - recall: 0.8535 - auc: 0.9128\n",
            "Epoch 294/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3239 - accuracy: 0.8333 - precision: 0.6467 - recall: 0.8506 - auc: 0.9132\n",
            "Epoch 295/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3248 - accuracy: 0.8262 - precision: 0.6348 - recall: 0.8475 - auc: 0.9129\n",
            "Epoch 296/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3239 - accuracy: 0.8264 - precision: 0.6340 - recall: 0.8531 - auc: 0.9131\n",
            "Epoch 297/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3248 - accuracy: 0.8292 - precision: 0.6389 - recall: 0.8528 - auc: 0.9128\n",
            "Epoch 298/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3247 - accuracy: 0.8278 - precision: 0.6377 - recall: 0.8464 - auc: 0.9129\n",
            "Epoch 299/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3237 - accuracy: 0.8271 - precision: 0.6358 - recall: 0.8499 - auc: 0.9133\n",
            "Epoch 300/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3240 - accuracy: 0.8304 - precision: 0.6414 - recall: 0.8506 - auc: 0.9131\n",
            "Epoch 301/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3231 - accuracy: 0.8278 - precision: 0.6365 - recall: 0.8520 - auc: 0.9137\n",
            "Epoch 302/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3244 - accuracy: 0.8272 - precision: 0.6359 - recall: 0.8499 - auc: 0.9129\n",
            "Epoch 303/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3238 - accuracy: 0.8302 - precision: 0.6410 - recall: 0.8510 - auc: 0.9132\n",
            "Epoch 304/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3235 - accuracy: 0.8291 - precision: 0.6383 - recall: 0.8549 - auc: 0.9133\n",
            "Epoch 305/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3219 - accuracy: 0.8335 - precision: 0.6459 - recall: 0.8559 - auc: 0.9145\n",
            "Epoch 306/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3229 - accuracy: 0.8282 - precision: 0.6374 - recall: 0.8510 - auc: 0.9138\n",
            "Epoch 307/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3224 - accuracy: 0.8296 - precision: 0.6388 - recall: 0.8566 - auc: 0.9140\n",
            "Epoch 308/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3220 - accuracy: 0.8297 - precision: 0.6387 - recall: 0.8577 - auc: 0.9142\n",
            "Epoch 309/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3230 - accuracy: 0.8280 - precision: 0.6371 - recall: 0.8506 - auc: 0.9136\n",
            "Epoch 310/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3220 - accuracy: 0.8306 - precision: 0.6410 - recall: 0.8538 - auc: 0.9140\n",
            "Epoch 311/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3225 - accuracy: 0.8300 - precision: 0.6402 - recall: 0.8528 - auc: 0.9140\n",
            "Epoch 312/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3219 - accuracy: 0.8288 - precision: 0.6382 - recall: 0.8528 - auc: 0.9143\n",
            "Epoch 313/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3220 - accuracy: 0.8310 - precision: 0.6431 - recall: 0.8482 - auc: 0.9142\n",
            "Epoch 314/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3211 - accuracy: 0.8311 - precision: 0.6418 - recall: 0.8552 - auc: 0.9147\n",
            "Epoch 315/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3211 - accuracy: 0.8290 - precision: 0.6377 - recall: 0.8570 - auc: 0.9147\n",
            "Epoch 316/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3215 - accuracy: 0.8301 - precision: 0.6418 - recall: 0.8464 - auc: 0.9145\n",
            "Epoch 317/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3212 - accuracy: 0.8321 - precision: 0.6434 - recall: 0.8556 - auc: 0.9146\n",
            "Epoch 318/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3206 - accuracy: 0.8309 - precision: 0.6416 - recall: 0.8542 - auc: 0.9150\n",
            "Epoch 319/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3215 - accuracy: 0.8275 - precision: 0.6358 - recall: 0.8531 - auc: 0.9146\n",
            "Epoch 320/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3208 - accuracy: 0.8312 - precision: 0.6413 - recall: 0.8581 - auc: 0.9150\n",
            "Epoch 321/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3202 - accuracy: 0.8330 - precision: 0.6456 - recall: 0.8531 - auc: 0.9149\n",
            "Epoch 322/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3202 - accuracy: 0.8313 - precision: 0.6425 - recall: 0.8535 - auc: 0.9152\n",
            "Epoch 323/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.3210 - accuracy: 0.8302 - precision: 0.6413 - recall: 0.8492 - auc: 0.9145\n",
            "Epoch 324/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3200 - accuracy: 0.8314 - precision: 0.6428 - recall: 0.8528 - auc: 0.9154\n",
            "Epoch 325/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3197 - accuracy: 0.8324 - precision: 0.6436 - recall: 0.8570 - auc: 0.9154\n",
            "Epoch 326/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3207 - accuracy: 0.8306 - precision: 0.6413 - recall: 0.8535 - auc: 0.9147\n",
            "Epoch 327/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3205 - accuracy: 0.8318 - precision: 0.6435 - recall: 0.8528 - auc: 0.9150\n",
            "Epoch 328/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3197 - accuracy: 0.8324 - precision: 0.6437 - recall: 0.8566 - auc: 0.9156\n",
            "Epoch 329/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3195 - accuracy: 0.8339 - precision: 0.6467 - recall: 0.8552 - auc: 0.9157\n",
            "Epoch 330/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3198 - accuracy: 0.8323 - precision: 0.6442 - recall: 0.8535 - auc: 0.9154\n",
            "Epoch 331/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3194 - accuracy: 0.8299 - precision: 0.6398 - recall: 0.8542 - auc: 0.9158\n",
            "Epoch 332/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3184 - accuracy: 0.8353 - precision: 0.6488 - recall: 0.8566 - auc: 0.9165\n",
            "Epoch 333/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3204 - accuracy: 0.8309 - precision: 0.6425 - recall: 0.8503 - auc: 0.9150\n",
            "Epoch 334/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3197 - accuracy: 0.8308 - precision: 0.6412 - recall: 0.8556 - auc: 0.9155\n",
            "Epoch 335/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3193 - accuracy: 0.8320 - precision: 0.6436 - recall: 0.8538 - auc: 0.9156\n",
            "Epoch 336/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3182 - accuracy: 0.8327 - precision: 0.6441 - recall: 0.8570 - auc: 0.9163\n",
            "Epoch 337/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3194 - accuracy: 0.8324 - precision: 0.6438 - recall: 0.8559 - auc: 0.9156\n",
            "Epoch 338/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3181 - accuracy: 0.8313 - precision: 0.6422 - recall: 0.8549 - auc: 0.9164\n",
            "Epoch 339/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3177 - accuracy: 0.8317 - precision: 0.6429 - recall: 0.8549 - auc: 0.9169\n",
            "Epoch 340/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3178 - accuracy: 0.8334 - precision: 0.6452 - recall: 0.8581 - auc: 0.9166\n",
            "Epoch 341/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3194 - accuracy: 0.8314 - precision: 0.6430 - recall: 0.8517 - auc: 0.9155\n",
            "Epoch 342/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3181 - accuracy: 0.8313 - precision: 0.6419 - recall: 0.8559 - auc: 0.9163\n",
            "Epoch 343/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3183 - accuracy: 0.8318 - precision: 0.6426 - recall: 0.8566 - auc: 0.9163\n",
            "Epoch 344/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3182 - accuracy: 0.8348 - precision: 0.6481 - recall: 0.8559 - auc: 0.9165\n",
            "Epoch 345/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3175 - accuracy: 0.8346 - precision: 0.6483 - recall: 0.8538 - auc: 0.9166\n",
            "Epoch 346/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3174 - accuracy: 0.8306 - precision: 0.6399 - recall: 0.8602 - auc: 0.9168\n",
            "Epoch 347/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3180 - accuracy: 0.8345 - precision: 0.6486 - recall: 0.8513 - auc: 0.9166\n",
            "Epoch 348/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3170 - accuracy: 0.8322 - precision: 0.6433 - recall: 0.8570 - auc: 0.9173\n",
            "Epoch 349/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3175 - accuracy: 0.8341 - precision: 0.6470 - recall: 0.8556 - auc: 0.9166\n",
            "Epoch 350/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3176 - accuracy: 0.8336 - precision: 0.6472 - recall: 0.8506 - auc: 0.9170\n",
            "Epoch 351/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3178 - accuracy: 0.8306 - precision: 0.6410 - recall: 0.8549 - auc: 0.9167\n",
            "Epoch 352/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3175 - accuracy: 0.8332 - precision: 0.6461 - recall: 0.8524 - auc: 0.9166\n",
            "Epoch 353/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3175 - accuracy: 0.8341 - precision: 0.6475 - recall: 0.8535 - auc: 0.9168\n",
            "Epoch 354/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3178 - accuracy: 0.8334 - precision: 0.6467 - recall: 0.8513 - auc: 0.9167\n",
            "Epoch 355/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3164 - accuracy: 0.8313 - precision: 0.6424 - recall: 0.8538 - auc: 0.9175\n",
            "Epoch 356/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3174 - accuracy: 0.8333 - precision: 0.6451 - recall: 0.8581 - auc: 0.9166\n",
            "Epoch 357/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3173 - accuracy: 0.8335 - precision: 0.6467 - recall: 0.8524 - auc: 0.9167\n",
            "Epoch 358/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3163 - accuracy: 0.8310 - precision: 0.6408 - recall: 0.8591 - auc: 0.9174\n",
            "Epoch 359/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3167 - accuracy: 0.8329 - precision: 0.6453 - recall: 0.8538 - auc: 0.9174\n",
            "Epoch 360/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3152 - accuracy: 0.8336 - precision: 0.6458 - recall: 0.8570 - auc: 0.9180\n",
            "Epoch 361/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3163 - accuracy: 0.8348 - precision: 0.6480 - recall: 0.8563 - auc: 0.9173\n",
            "Epoch 362/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3163 - accuracy: 0.8348 - precision: 0.6482 - recall: 0.8556 - auc: 0.9176\n",
            "Epoch 363/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3157 - accuracy: 0.8349 - precision: 0.6482 - recall: 0.8563 - auc: 0.9177\n",
            "Epoch 364/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3182 - accuracy: 0.8336 - precision: 0.6468 - recall: 0.8528 - auc: 0.9161\n",
            "Epoch 365/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3160 - accuracy: 0.8351 - precision: 0.6481 - recall: 0.8584 - auc: 0.9178\n",
            "Epoch 366/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3157 - accuracy: 0.8347 - precision: 0.6470 - recall: 0.8602 - auc: 0.9178\n",
            "Epoch 367/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3149 - accuracy: 0.8366 - precision: 0.6521 - recall: 0.8531 - auc: 0.9183\n",
            "Epoch 368/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3149 - accuracy: 0.8345 - precision: 0.6476 - recall: 0.8559 - auc: 0.9182\n",
            "Epoch 369/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3146 - accuracy: 0.8319 - precision: 0.6422 - recall: 0.8595 - auc: 0.9184\n",
            "Epoch 370/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3151 - accuracy: 0.8349 - precision: 0.6485 - recall: 0.8549 - auc: 0.9182\n",
            "Epoch 371/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3156 - accuracy: 0.8344 - precision: 0.6468 - recall: 0.8588 - auc: 0.9180\n",
            "Epoch 372/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3150 - accuracy: 0.8344 - precision: 0.6462 - recall: 0.8616 - auc: 0.9182\n",
            "Epoch 373/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3146 - accuracy: 0.8362 - precision: 0.6512 - recall: 0.8538 - auc: 0.9182\n",
            "Epoch 374/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3154 - accuracy: 0.8320 - precision: 0.6438 - recall: 0.8528 - auc: 0.9181\n",
            "Epoch 375/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3161 - accuracy: 0.8329 - precision: 0.6455 - recall: 0.8524 - auc: 0.9174\n",
            "Epoch 376/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3143 - accuracy: 0.8351 - precision: 0.6480 - recall: 0.8588 - auc: 0.9184\n",
            "Epoch 377/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3141 - accuracy: 0.8374 - precision: 0.6524 - recall: 0.8577 - auc: 0.9186\n",
            "Epoch 378/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3146 - accuracy: 0.8325 - precision: 0.6441 - recall: 0.8552 - auc: 0.9184\n",
            "Epoch 379/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3137 - accuracy: 0.8326 - precision: 0.6434 - recall: 0.8595 - auc: 0.9187\n",
            "Epoch 380/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3136 - accuracy: 0.8399 - precision: 0.6574 - recall: 0.8563 - auc: 0.9190\n",
            "Epoch 381/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3137 - accuracy: 0.8348 - precision: 0.6483 - recall: 0.8552 - auc: 0.9190\n",
            "Epoch 382/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3145 - accuracy: 0.8354 - precision: 0.6483 - recall: 0.8605 - auc: 0.9187\n",
            "Epoch 383/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3143 - accuracy: 0.8353 - precision: 0.6479 - recall: 0.8609 - auc: 0.9186\n",
            "Epoch 384/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3137 - accuracy: 0.8322 - precision: 0.6425 - recall: 0.8605 - auc: 0.9189\n",
            "Epoch 385/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3139 - accuracy: 0.8372 - precision: 0.6522 - recall: 0.8573 - auc: 0.9187\n",
            "Epoch 386/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3139 - accuracy: 0.8350 - precision: 0.6480 - recall: 0.8581 - auc: 0.9189\n",
            "Epoch 387/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3132 - accuracy: 0.8331 - precision: 0.6444 - recall: 0.8595 - auc: 0.9193\n",
            "Epoch 388/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3126 - accuracy: 0.8357 - precision: 0.6486 - recall: 0.8616 - auc: 0.9196\n",
            "Epoch 389/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3134 - accuracy: 0.8357 - precision: 0.6491 - recall: 0.8595 - auc: 0.9191\n",
            "Epoch 390/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3133 - accuracy: 0.8361 - precision: 0.6511 - recall: 0.8538 - auc: 0.9190\n",
            "Epoch 391/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3129 - accuracy: 0.8366 - precision: 0.6509 - recall: 0.8584 - auc: 0.9194\n",
            "Epoch 392/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3140 - accuracy: 0.8340 - precision: 0.6479 - recall: 0.8506 - auc: 0.9185\n",
            "Epoch 393/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3126 - accuracy: 0.8350 - precision: 0.6475 - recall: 0.8602 - auc: 0.9197\n",
            "Epoch 394/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3124 - accuracy: 0.8355 - precision: 0.6486 - recall: 0.8602 - auc: 0.9197\n",
            "Epoch 395/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3132 - accuracy: 0.8340 - precision: 0.6455 - recall: 0.8616 - auc: 0.9191\n",
            "Epoch 396/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3138 - accuracy: 0.8340 - precision: 0.6456 - recall: 0.8612 - auc: 0.9191\n",
            "Epoch 397/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3136 - accuracy: 0.8347 - precision: 0.6491 - recall: 0.8510 - auc: 0.9189\n",
            "Epoch 398/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3126 - accuracy: 0.8353 - precision: 0.6489 - recall: 0.8570 - auc: 0.9195\n",
            "Epoch 399/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3136 - accuracy: 0.8321 - precision: 0.6433 - recall: 0.8559 - auc: 0.9187\n",
            "Epoch 400/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.3119 - accuracy: 0.8348 - precision: 0.6469 - recall: 0.8616 - auc: 0.9199\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49cKTgygS2Sq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8cc5b43-15a6-4a22-dbfb-ebb55b3bb1ff"
      },
      "source": [
        "#@title Evaluate test set\n",
        "\n",
        "features = {name:np.array(value) for name, value in test_df.items()}\n",
        "label = np.array(features.pop(label_name))\n",
        "\n",
        "my_model.evaluate(x = features, y = label, batch_size=batch_size)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'parent_protein_id': <tf.Tensor 'IteratorGetNext:14' shape=(None,) dtype=string>, 'protein_seq': <tf.Tensor 'IteratorGetNext:20' shape=(None,) dtype=string>, 'start_position': <tf.Tensor 'IteratorGetNext:23' shape=(None,) dtype=int64>, 'end_position': <tf.Tensor 'IteratorGetNext:6' shape=(None,) dtype=int64>, 'peptide_seq': <tf.Tensor 'IteratorGetNext:19' shape=(None,) dtype=string>, 'chou_fasman': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=float32>, 'emini': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=float32>, 'kolaskar_tongaonkar': <tf.Tensor 'IteratorGetNext:12' shape=(None,) dtype=float32>, 'parker': <tf.Tensor 'IteratorGetNext:15' shape=(None,) dtype=float32>, 'isoelectric_point': <tf.Tensor 'IteratorGetNext:10' shape=(None,) dtype=float32>, 'aromaticity': <tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=float32>, 'hydrophobicity': <tf.Tensor 'IteratorGetNext:8' shape=(None,) dtype=float32>, 'stability': <tf.Tensor 'IteratorGetNext:21' shape=(None,) dtype=float32>, 'peptide_length': <tf.Tensor 'IteratorGetNext:17' shape=(None,) dtype=int64>, 'hs_ratio': <tf.Tensor 'IteratorGetNext:7' shape=(None,) dtype=float32>, 'chou_fasman_norm': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=float32>, 'emini_norm': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=float32>, 'kolaskar_tongaonkar_norm': <tf.Tensor 'IteratorGetNext:13' shape=(None,) dtype=float32>, 'parker_norm': <tf.Tensor 'IteratorGetNext:16' shape=(None,) dtype=float32>, 'isoelectric_point_norm': <tf.Tensor 'IteratorGetNext:11' shape=(None,) dtype=float32>, 'aromaticity_norm': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float32>, 'hydrophobicity_norm': <tf.Tensor 'IteratorGetNext:9' shape=(None,) dtype=float32>, 'stability_norm': <tf.Tensor 'IteratorGetNext:22' shape=(None,) dtype=float32>, 'peptide_length_norm': <tf.Tensor 'IteratorGetNext:18' shape=(None,) dtype=float32>, 'start_position_norm': <tf.Tensor 'IteratorGetNext:24' shape=(None,) dtype=float32>}. Consider rewriting this model with the Functional API.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45/45 [==============================] - 1s 3ms/step - loss: 0.3895 - accuracy: 0.8026 - precision: 0.6007 - recall: 0.8058 - auc: 0.8791\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.38947999477386475,\n",
              " 0.8025933504104614,\n",
              " 0.6007393598556519,\n",
              " 0.8057851195335388,\n",
              " 0.8790900111198425]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}